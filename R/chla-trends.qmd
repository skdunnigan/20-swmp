---
title: "Trends in Chlorophyll"
format: 
  html:
    toc: true
    code-fold: true
    embed-resources: true
execute: 
  warning: false
  message: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r}
#| label: setup
#| include: false
if(!require(here)){ install.packages("here") } ;  library(here) # easy paths
if(!require(dplyr)){ install.packages("dplyr") } ;  library(dplyr) # left_join
if(!require(tidyr)){ install.packages("tidyr") } ;  library(tidyr) # pipe operator %>% 
if(!require(ggplot2)){install.packages("ggplot2")} ; library(ggplot2) # plotting
if(!require(lubridate)){ install.packages("lubridate") } ;  library(tidyr) # pipe operator %>% 
if(!require(janitor)){ install.packages("janitor") } ;  library(janitor) # clean names
if(!require(readxl)){ install.packages("readxl") } ;  library(readxl) # clean names
if(!require(wqtrends)){ 
  options(repos = c(
  tbeptech = 'https://tbep-tech.r-universe.dev',
  CRAN = 'https://cloud.r-project.org'))
  install.packages('wqtrends')} ; library(wqtrends) # models and plots for trends

```

# Data

## Collection Methods

Water samples were collected monthly at each of the four water quality monitoring stations at the GTMNERR for nitrite + nitrate, ammonium, total nitrogen, orthophosphate, total phosphorus, chlorophyll a, and total suspended solids using two types of sampling methods (grab and diel), on the same day during an ebb tide. Grab samples were collected from all stations in duplicate using a pole sampler and bucket. Diel samples were only collected at the Pellicer Creek station at 2.5-hour intervals for a 25-hour (lunar day) period using a Teledyne ISCO automatic sampler equipped with an internal ice bath to keep samples cold. Both grab and diel samples were collected from the approximate depth of the sonde sensors, 1 m above the bottom. Samples were filtered in the field whenever feasible; otherwise, they were placed on ice in the dark and filtered immediately upon returning to the laboratory.

Water samples collected between January 1, 2003 and June 30, 2010 were collected and analyzed by University of Florida's (UF) Department of Fisheries and Aquatic Sciences. Samples collected between July 1, 2010 and November 30, 2012 were collected by GTMNERR and analyzed by the UF lab. Samples collected on or after December 1, 2012 were collected by GTMNERR and analyzed by the Florida Department of Environmental Protection's (FDEP) Central Laboratory. Both laboratories were certified through the National Environmental Laboratory Accreditation Program and used the American Public Health Association and Environmental Protection Agency methods for analyses. Method details can be found in annual metadata files at www.nerrsdata.org.

## The data set

Loading the nutrient data file and cleaning it up for analysis. This file is an appended internal file to the GTMNERR that contains data from 2002-2023 with QC flags and codes and all the parameters included. This data is not included in the GitHub repository, but can be requested from the GTMNERR.

```{r}
#| label: load-data
#| echo: false

nms <- names(read_excel(here::here('data',
                                   'All_inclusive_NUT',
                                   'gtmnut2002-2023_QC_zeros-corrected.xlsx'), 
                        n_max = 0)) # pull out all the column names in this file

class <- ifelse(grepl("^F_", nms), "text", "numeric") # read everything with F_ as a character
class2 <- class[-(1:5)] # remove the first five elements of the vector because they are different

NUT <- readxl::read_xlsx(here::here('data',
                                    'All_inclusive_NUT',
                                    'gtmnut2002-2023_QC_zeros-corrected.xlsx'),
                         col_types = c("text", 
                                       "date", 
                                       "numeric", 
                                       "numeric", 
                                       "text", 
                                       class2)) %>% # specify how to read in these columns
  janitor::clean_names()

# clean environment
rm(nms, class, class2)

glimpse(NUT)

```

# Data preparation

## Selecting and filtering QAQC flags and codes

We are interested in examining trends in chlorophyll *a* (chl-a) for complete years across a 20 year period. Therefore, we are only selecting the chl-a data from 2003-2022 at all stations. This dataset includes both grab and diel collection results, so the diel data will be filtered out as well.

```{r}
#| label: chla-dat

chla <- NUT %>% 
          filter(!is.na(rep)) %>% # remove "S" reps in dataset
          select(station_code, 2:5, chla_n, f_chla_n) %>% # keep only chla data
          rename(datetimestamp = date_time_stamp) %>% # clean name
          mutate(date = lubridate::date(datetimestamp)) %>% # create date variable from POSIXct
          filter(monitoring_program == 1) %>% # keep only grab samples (remove DIEL)
          select(-monitoring_program, -rep) # remove columns once done

glimpse(chla)

```

The F_Record column contains [QAQC codes](https://cdmo.baruch.sc.edu/data/qaqc.cfm) that apply to the entire record. We want to look at what all are included in this dataset.

```{r}
# examine f_record values for further filtering
unique(chla$f_record)

```

All of those codes are okay, none of the records require further filtering. Next will be to look at the flags and codes for the chl-a results:

```{r}
# examine f_chla values for further filtering
unique(chla$f_chla_n)

```

This data contains several records that will not be good to keep for the analysis. Examining the suspect data ("\<1\>") it seems a lot of that data contains CSM codes and metadata documentation that says these reference "Laboratory indicated that the values reported are the mean of two or more determinations" and values were estimated. For now, all rejected and suspect data will be removed for analysis. I also removed the data flagged with "\<-4\>" which means "below sensor limit". This means this data was below the minimum detection limit from lab.

### Nominal base MDL

There are also quite a few very low values that occur early on in the timeseries:

```{r}
head(chla %>% arrange(chla_n))
```

These values fall below the nominal base minimum detection limit (MDL) for chl-a used in the last 10 years of the record which is 0.55($\mu$g/L). Therefore, any values less than this MDL will be set to that MDL. This will apply to these records:

```{r}
chla %>% filter(chla_n < 0.55)
```

This will replace all these values with the nominal base MDL of 0.55($\mu$g/L).

```{r}
chla$chla_n[chla$chla_n < 0.55] <- 0.55 # replace all values below nominal base mdl of 0.55 with the base mdl 0.55
```

## Formatting for analysis

So, filtering the data to remove unusable QC data, then averaging the duplicates for a monthly average chl-a at each site (`value`), and then adding information to match the format used for the `wqtrends` [workflow](https://tbep-tech.github.io/wqtrends/articles/introduction.html) [@beck2022]. This means adding a few columns such as `doy` (day of year), `cont_year` (date in decimal time), `yr` (year), `mo` (month as character label), and `param` which is probably not necessary, but since it was included in the vignette, I added a column to make it clear this is chl-a data. This is also where I filtered to remove 2002 and 2023 data since they are not of interest for this trend analysis.

```{r}
# prep data for GAM

chla_dat <- chla %>% 
              filter(!grepl(c("<-3>"), f_chla_n) & 
                       !grepl(c("<1>"), f_chla_n) &
                        !grepl(c("<-4>"), f_chla_n)) %>% # remove rejected and suspect data
              select(-f_record, -f_chla_n) %>% # remove qc columns after filtering
              group_by(station_code, date) %>% # group by station and date to average duplicates
              summarise(value = mean(chla_n, na.rm = T)) %>% # avg duplicates
              ungroup() %>% 
              mutate(doy = lubridate::yday(date), # day of the year
                     cont_year = lubridate::decimal_date(date), # date in decimal time
                     yr = lubridate::year(date), # year
                     mo = lubridate::month(date, label = TRUE), # month
                     param = "chla") %>% # add param name
              rename(station = station_code) %>% # clean variable name
              filter(yr > 2002 & yr < 2023) # only keep data from 2003-2022

head(chla_dat)
```

# Data Analysis

## Data distribution

Look at distribution of data points using the `ggdist` package along with `ggplot2` to plot out the bell shape along with the distribution of points (underneath). The data is highly skewed.

```{r}
chla_dat %>% 
  mutate(station = factor(station, 
                          levels = c("gtmpcnut",
                                     "gtmfmnut",
                                     "gtmssnut",
                                     "gtmpinut"))) %>% 
  ggplot(aes(x = value, y = station, fill = station)) +
  ggdist::stat_slab(aes(thickness = after_stat(pdf*n)), scale = 0.7) +
  ggdist::stat_dotsinterval(side = "bottom", scale = 0.7, slab_linewidth = NA) +
  scale_fill_brewer(palette = "Set2") +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Chlorophyll-a (\U00B5g/L)",
       y = "",
       title = "Identity") 
```

Replotted the data with a log-10 transformation of the data. The data looks much better, although gtmpcnut is still curious. 

```{r}
chla_dat %>% 
  mutate(station = factor(station, 
                          levels = c("gtmpcnut",
                                     "gtmfmnut",
                                     "gtmssnut",
                                     "gtmpinut"))) %>% 
  ggplot(aes(x = log10(value), y = station, fill = station)) +
  ggdist::stat_slab(aes(thickness = after_stat(pdf*n)), scale = 0.7) +
  ggdist::stat_dotsinterval(side = "bottom", scale = 0.7, slab_linewidth = NA) +
  scale_fill_brewer(palette = "Set2") +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Chlorophyll-a (\U00B5g/L)",
       y = "",
       title = "Log-10 Transformation") 
```

## Q-Q plots

The data should fall along a straight diagonal line.

```{r}
pi <- chla_dat %>% filter(station == "gtmpinut") %>% mutate(chla_t = log10(value))
ss <- chla_dat %>% filter(station == "gtmssnut") %>% mutate(chla_t = log10(value))
fm <- chla_dat %>% filter(station == "gtmfmnut") %>% mutate(chla_t = log10(value))
pc <- chla_dat %>% filter(station == "gtmpcnut") %>% mutate(chla_t = log10(value))

```


::: {.panel-tabset group = "site"}

### Pine Island

```{r}
par(mfrow=c(1,2)) 
qqnorm(pi$value, main ="gtmpinut")
qqline(pi$value)

qqnorm(pi$chla_t, main ="gtmpinut log10")
qqline(pi$chla_t)
```

### San Sebastian

```{r}
par(mfrow=c(1,2)) 
qqnorm(ss$value, main ="gtmssnut")
qqline(ss$value)

qqnorm(ss$chla_t, main ="gtmssnut log10")
qqline(ss$chla_t)
```

### Fort Matanzas

```{r}
par(mfrow=c(1,2)) 
qqnorm(fm$value, main ="gtmfmnut")
qqline(fm$value)

qqnorm(fm$chla_t, main ="gtmfmnut log10")
qqline(fm$chla_t)
```

### Pellicer Creek

gtmpcnut has a non-normal distribution, even after the log-10 transformation.

```{r}
par(mfrow=c(1,2)) 
qqnorm(pc$value, main ="gtmpcnut")
qqline(pc$value)

qqnorm(pc$chla_t, main ="gtmpcnut log10")
qqline(pc$chla_t)

```

:::

## Normality Testing

Shapiro-Wilk Normality Tests on log-10 transformed data

```{r}
shapiro.test(pi$chla_t)
shapiro.test(ss$chla_t)
shapiro.test(fm$chla_t)
shapiro.test(pc$chla_t)

ks.test(pc$chla_t, 'pnorm')

rm(pi, ss, fm, pc)
```

Pellicer Creek ("gtmpcnut") fails normality tests (Shapiro-Wilk and Kolmogorov-Smirnov Tests) even after log10 transformations.

## Trends

From the vignette for `wqtrends`, the generalized additive model (GAM) fits additive smoothing functions to describe variations in the response variable `value` over time, which is measured as a continuous number in `cont_year`. The GAM used in this model is 

chl \~ s(cont_year, k = *large*)

and therefore `anlz_gam()` is used to fit the model. The upper limit of `k` (number of knots) *"should be large enough to allow sufficient flexibility in the smoothing term"* and is recommended as 12 times the number of years for the input data, however the number of knots can decrease until the data can be modeled (if insufficent data are available). 

::: {.panel-tabset}

### Pine Island

```{r}
tomod <- chla_dat %>%
          filter(station %in% "gtmpinut") # only keep pine island data

mod <- wqtrends::anlz_gam(tomod, trans = "log10")
mod
```

"GCV" is the generalized cross-validation score, which minimizes out-of-sample prediction error. It penalizes the net curve of a spline.

Now to assess fit of the model by using `anlz_smooth()` to assess individual smoother functions and `anlz_fit()` for overall fit.

```{r}
anlz_smooth(mod)

anlz_fit(mod)
```

Using the basic smoother with just `cont_year` as a variable, the test statistics are okay. The AIC is low and there is a decent R2 value.

Estimate results by day of year

```{r}
ylab <- "Chlorophyll-a (\U00B5g/L)"
show_prddoy(mod, ylab = ylab) + labs(caption = "gtmpinut")

```

Predictions for the model across the entire time series. Points are the observed data and the lines are the predicted. I have modified the `show_prdseries()` function from the `wqtrends` package to `show_prdseries_mod()` for basic aesthetic changes.

```{r}
#| label: fxn-mod1

show_prdseries_mod <-  function (mod, ylab, alpha = 0.7, base_size = 11, xlim = NULL, 
    ylim = NULL, site) 
{
    prds <- anlz_prd(mod)
    trans <- unique(prds$trans)
    tobacktrans <- mod$model %>% dplyr::mutate(trans = mod$trans)
    moddat <- anlz_backtrans(tobacktrans) %>% dplyr::mutate(date = lubridate::date_decimal(cont_year), 
        date = as.Date(date))
    p <- ggplot2::ggplot(prds, ggplot2::aes(x = date)) + 
          ggplot2::geom_point(data = moddat, 
                    ggplot2::aes(y = value), color = "gray75", size = 1) + 
      ggplot2::geom_line(ggplot2::aes(y = value), 
        linewidth = 0.75, alpha = alpha, colour = "#56B4E9") + 
      ggplot2::theme_bw(base_family = "serif", 
        base_size = base_size) + 
      ggplot2::theme(legend.position = "top", 
        legend.title = ggplot2::element_blank(), 
        axis.title.x = ggplot2::element_blank(),
        axis.text = element_text(size = 12, color = "black")) +
      ggplot2::labs(y = ylab,
                    title = site) + 
      ggplot2::coord_cartesian(xlim = xlim, ylim = ylim)
    if (trans != "ident") 
        p <- p + ggplot2::scale_y_log10()
    return(p)
}
```

```{r}
show_prdseries_mod(mod, ylab = "Chlorophyll-a (\U00B5g/L)", site = "gtmpinut")
```

```{r}
show_prdseason(mod, ylab = ylab) + labs(caption = "gtmpinut")
```

```{r}
#| echo: false

# first 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2012, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 6.6, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2013, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 6.6, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 5 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2018, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 6.6, linetype = "dashed", linewidth = 1, color = "gray75") 

# record
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 6.6, linetype = "dashed", linewidth = 1, color = "gray75") 

```

### San Sebastian

```{r}
tomod <- chla_dat %>%
          filter(station %in% "gtmssnut") # only keep pine island data

mod <- wqtrends::anlz_gam(tomod, trans = "log10")
mod
```

Now to assess fit of the model by using `anlz_smooth()` to assess individual smoother functions and `anlz_fit()` for overall fit.

```{r}
anlz_smooth(mod)
```

from the help: *"the estimated degrees of freedom (`edf`), the reference degrees of freedom (`Ref.df`), the test statistic (`F`), and significance of the component (`p-value`). The significance of the component is in part based on the difference between `edf` and `Ref.df`."* 

```{r}
anlz_fit(mod)
```

Estimate results by day of year

```{r}
ylab <- "Chlorophyll-a (\U00B5g/L)"
show_prddoy(mod, ylab = ylab) + labs(caption = "gtmssnut")

```

Predictions for the model across the entire time series. Points are the observed data and the lines are the predicted.

```{r}
show_prdseries_mod(mod, ylab = "Chlorophyll-a (\U00B5g/L)", site = "gtmssnut")
```

```{r}
show_prdseason(mod, ylab = ylab) + labs(caption = "gtmssnut")
```

```{r}
#| echo: false

# first 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2012, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.0, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2013, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.0, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 5 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2018, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.0, linetype = "dashed", linewidth = 1, color = "gray75") 

# record
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.0, linetype = "dashed", linewidth = 1, color = "gray75") 

```

### Fort Matanzas

```{r}
tomod <- chla_dat %>%
          filter(station %in% "gtmfmnut") # only keep pine island data

mod <- wqtrends::anlz_gam(tomod, trans = "log10")
mod
```

Now to assess fit of the model by using `anlz_smooth()` to assess individual smoother functions and `anlz_fit()` for overall fit.

```{r}
anlz_smooth(mod)
```

from the help: *"the estimated degrees of freedom (`edf`), the reference degrees of freedom (`Ref.df`), the test statistic (`F`), and significance of the component (`p-value`). The significance of the component is in part based on the difference between `edf` and `Ref.df`."* 

```{r}
anlz_fit(mod)
```

Estimate results by day of year

```{r}
ylab <- "Chlorophyll-a (\U00B5g/L)"
show_prddoy(mod, ylab = ylab) + labs(caption = "gtmfmnut")

```

Predictions for the model across the entire time series. Points are the observed data and the lines are the predicted.

```{r}
show_prdseries_mod(mod, ylab = "Chlorophyll-a (\U00B5g/L)", site = "gtmfmnut")
```

```{r}
show_prdseason(mod, ylab = ylab) + labs(caption = "gtmfmnut")
```

```{r}
#| echo: false

# first 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2012, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 5.5, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2013, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 5.5, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 5 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2018, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 5.5, linetype = "dashed", linewidth = 1, color = "gray75") 

# record
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 5.5, linetype = "dashed", linewidth = 1, color = "gray75") 

```

### Pellicer Creek

*did not meet normality assumptions*

```{r}
tomod <- chla_dat %>%
          filter(station %in% "gtmpcnut") # only keep pine island data

mod <- wqtrends::anlz_gam(tomod, trans = "log10")
mod
```

Now to assess fit of the model by using `anlz_smooth()` to assess individual smoother functions and `anlz_fit()` for overall fit.

```{r}
anlz_smooth(mod)
```

from the help: *"the estimated degrees of freedom (`edf`), the reference degrees of freedom (`Ref.df`), the test statistic (`F`), and significance of the component (`p-value`). The significance of the component is in part based on the difference between `edf` and `Ref.df`."* 

```{r}
anlz_fit(mod)
```

Estimate results by day of year

```{r}
ylab <- "Chlorophyll-a (\U00B5g/L)"
show_prddoy(mod, ylab = ylab) + labs(caption = "gtmpcnut")

```

Predictions for the model across the entire time series. Points are the observed data and the lines are the predicted.

```{r}
show_prdseries_mod(mod, ylab = "Chlorophyll-a (\U00B5g/L)", site = "gtmpcnut")
```

```{r}
show_prdseason(mod, ylab = ylab) + labs(caption = "gtmpcnut")
```

```{r}
#| echo: false

# first 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2012, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.3, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 10 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2013, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.3, linetype = "dashed", linewidth = 1, color = "gray75") 

# last 5 years
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2018, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.3, linetype = "dashed", linewidth = 1, color = "gray75") 

# record
show_metseason(mod, doystr = 1, doyend = 365, yrstr = 2003, yrend = 2022, ylab = "Chlorophyll-a (ug/L)") + geom_hline(yintercept = 4.3, linetype = "dashed", linewidth = 1, color = "gray75") 

```

:::

## Variability

Using methods from @cloern2010, variability and patterns in the monthly chlorophyll *a* data were further extracted by decomposing the timeseries at each station using Equation 4 into an annual effect, mean seasonal pattern, and residual "events".

::: callout-note
### Equation 4 from @cloern2010:

$$
c_{ij}=Cy_im_j\epsilon_{ij}
$$ Where $c_{ij}$ is chlorophyll concentration in year $i$($i=1,...,N$) and month $j$($j=1,...12$); *C* is the long-term mean of the series; $y_i$ is the annual effect in the *i*th year; $m_j$ is the seasonal (monthly) effect in the *j*th month; and $\epsilon_{ij}$ is the residual.

-   If $y_i$ \>1, indicates a year with above-average mean Chl-*a*.
-   If $m_j$ \>1, indicates a mean monthly Chl-*a* greater than the annual mean.
-   If $\epsilon_{ij}$ \>1, indicates an observation greater than the expected value for that month and year.
:::

```{r}
#| label: decomp-fxn
#| include: false

# function comes from Cloern and Jassby 2010 Supplemental Material

decomp.mult <- function(x, startyr = NULL , endyr = NULL, event = T){
#R2 .8.0 3/2/09 4:48 PM
  if(class(x)[1]!='mts') dim(x)=c(length(x),1)
  
  if(is.null(startyr)) startyr <- start(x)[1]
  
  if(is.null(endyr)) endyr <- end(x)[1]
  
  d <- window(x, start = c(startyr,1), end = c(endyr,12), extend = T)
  
  results <- vector('list',dim(d)[2])
  
  names(results)= colnames(d)
  
  for(site in colnames (d)){
    d1=d[,site]
    
    #long-term mean
    grandmean = mean(d1, na.rm = T)
    
    # annual component
    annualmean = aggregate(d1,1, mean, na.rm = T)
    annualmeanreps = as.vector(t(matrix(rep(annualmean,12), 
                                        ncol = 12)))
    interann = ts(annualmeanreps,s=c(startyr,1),f = 12) / grandmean
    # remaining components
    if(event) {
      # monthly component
      d2 = matrix(d1, nrow = 12)
      monthdev = sweep(d2,2, annualmean ,'/')
      monthmean = apply(monthdev,1, mean , na.rm = T)
      season = ts(rep(monthmean, endyr - startyr + 1), 
                  s=c(startyr, 1), f = 12)
      # events component
      resids = sweep(monthdev, 1, monthmean , '/')
      events = ts(as.vector(resids),
                  s=c(startyr, 1),f = 12)
    }
    else {
      # monthly component
      season = d1/(grandmean * interann)
    }
    # prepare output
    if(event) dcomp = ts.union(d1, grandmean, interann, season,
      events) else dcomp =ts.union (d1, grandmean, interann,
      season)
    colnames(dcomp)[1]= 'original'
    results[[site]]= dcomp
    }
    if(class(x)[1]!='mts') results[[1]] else results
}
```

In order to decompose the timeseries, there cannot be any gaps in the data. I need to identify gaps and fill them with NAs

```{r}
chla_dat %>% 
  filter(station == "gtmpinut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmpinut needs NAs placed for 2005-01, 06-2009, 02-2010, 08-2015, 08-2020.

```{r}
chla_dat %>% 
  filter(station == "gtmssnut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmssnut needs NAs placed for 2005-01, 06-2009, 02-2010, 09-2019, 08-2020, 07-2022

```{r}
chla_dat %>% 
  filter(station == "gtmfmnut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmfmnut needs NAs placed for 2005-01, 06-2009, 02-2010, 11-2015

```{r}
chla_dat %>% 
  filter(station == "gtmpcnut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmpcnut needs NAs placed for 2003-01, 2005-01, 2009-09, 2010-02, 2013-07, 2015-08, 2015-09, 2018-11, 2020-07, 2022-04, 2022-07, 2022-08

*this may be why the data is not normally distributed...too much is missing. May need to see if I can use the ISCO data to fill in the missing information.*



```{r}
#| label: dat-ts
#| include: false

# 2005-01-01 is missing from all four sites, need to build in a placeholder for it
mis <- tribble(
  ~date, ~value,
  "2005-01-01", NA
) %>% mutate(date = as.Date(date),
             value = as.numeric(value))
# pc is missing 2003-01 and 2005-01
ts_chla <- function(station){
  ts(as.data.frame(chla_dat %>% 
                              filter(station == {{station}}) %>%
                              select(date, value) %>% 
                              bind_rows(mis) %>% 
                              arrange(date) %>% 
                              select(value)),
              start = c(2003,1), # sampling started 2002-05, but 2003 is first complete year
              end = c(2022,12),
              frequency = 12) # monthly
  
}

# create timeseries objects of chlorophyll data
chla_pi <- ts(as.data.frame(chla_dat %>% 
                              filter(station == "gtmssnut") %>%
                              select(date, value) %>% 
                              # bind_rows(mis) %>% 
                              arrange(date) %>% 
                              select(value)),
              start = c(2003,1), # sampling started 2002-05, but 2003 is first complete year
              end = c(2022,12),
              frequency = 12) # monthly

# ss missing 2005-01, 2009-06, 2010-02, 2020-08, 2022-07

chla_ss <- ts(as.data.frame(chla_dat %>% 
                              filter(station == "gtmpinut") %>%
                              select(date, value) %>% 
                              bind_rows(mis) %>% 
                              arrange(date) %>% 
                              select(value)),
              start = c(2003,1), # sampling started 2002-05, but 2003 is first complete year
              end = c(2022,12),
              frequency = 12) # monthly

chla_fm <- ts(as.data.frame(fm_nut_mo %>% 
                              filter(datetimestamp > "2002-12-31" &
                                       datetimestamp < "2022-01-01") %>%
                              select(datetimestamp, chla_n) %>% 
                              bind_rows(mis) %>% 
                              arrange(datetimestamp) %>% 
                              select(chla_n)),
              start = c(2003,1), # sampling started 2002-05, but 2003 is first complete year
              end = c(2022,12), 
              frequency = 12) # monthly

chla_pc <- ts(as.data.frame(pc_nut_mo %>% 
                              filter(datetimestamp > "2002-12-31" &
                                       datetimestamp < "2022-01-01") %>%
                              select(datetimestamp, chla_n) %>% 
                              bind_rows(mis) %>% 
                              arrange(datetimestamp) %>% 
                              select(chla_n)),
              start = c(2003,1), # sampling started 2002-05, but 2003 is first complete year
              end = c(2022,12),
              frequency = 12) # monthly

rm(mis)
```